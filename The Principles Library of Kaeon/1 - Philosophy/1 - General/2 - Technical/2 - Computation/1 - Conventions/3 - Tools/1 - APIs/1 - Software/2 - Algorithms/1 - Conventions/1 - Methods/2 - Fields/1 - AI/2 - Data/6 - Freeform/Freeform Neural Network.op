Freeform Neural Network

	Philosophy

		-
			A freeform neural network, or FFNN, is a weighted directed graph where each node stores
			a number, that operates similarly to a feed-forward neural network, and which can be
			used as a machine learning architecture.
		-

	Principles

		Theory

			Step

				-
					An FFNN step is where, for every node in the graph, the number that was in the
					node at the beginning of the step is sent to across each connection of said
					node to the node at the end of said connection, being multiplied by the weight
					of said connection before reaching the latter node.

					Once this has been done for all nodes and their connections, each node shall
					take all of the numbers it has received, add them together, and run them
					through a function, generating a value that shall replace the number within the
					node.

					Said function should, by default, be a sigmoid function.

					A partial step is a step where only some nodes are processed.
				-

			Usage

				Cycles

					-
						In the usage of an FFNN as a machine learning system, certain nodes of the
						graph shall be arbitrarily selected as inputs, and certain nodes shall be
						arbitrarily selected as outputs.

						A cycle shall consist of a sequence of steps which shall be executed until
						either completing a set number of steps or reaching a satisfactory result
						in the output nodes. During a cycle, values may be manually set in the
						input nodes by external systems. The nodes selected as inputs and outputs
						may change during a cycle.
					-

				Override

					-
						An override is where nodes other than input nodes have their values
						manually set.

						An FFNN flush is an FFNN override where the value of each node is set to
						zero.

						It is not necessary to flush an FFNN between uses, and not doing so can
						allow the FFNN to function as memory, which it may utilize in future
						cycles.
					-

				Linearization

					-
						An FFNN cycle may be linearized by converting the beginning and end of each
						step within it into a layer in a feed forward neural network, the end of
						one step being the start of the next.
						
						In a linearized FFNN cycle, nodes and connections may repeat as shallow
						copies of themselves throughout the network.
						
						Once in this format, the linearized FFNN cycle may be trained using some
						variation of backpropagation, after which a newly trained FFNN model may be
						reconstructed from the trained linearized FFNN cycle. This process is
						called linearized training.
					-
			
			Training

				-
					In addition to linearized training, FFNN training may involve the adding and
					pruning of nodes and connections, processes which aim to minimize steps as well
					as error, breeding via genetic algorithms, which may involve genetic
					serialization.

					Additionally, linearized testing may utilize correlation training.
				-

		Application

			Notes

				Types

					XNN

						-
							A cross neural network, or XNN, is an FFNN where the network is a dual
							absolute graph.
						-

				Uses

					AGI

						-
							Because FFNNs may be used to host multiple machine learning contexts
							simultaneously, they, especially XNNs, may be the ideal architecture
							for AGI.
						-

			Conventions

				-
					An FFNN convention is a codified protocol for using and training FFNNs in
					certain contexts.
				-