Freeform Neural Network

	Philosophy

		-
			A freeform neural network, or FFNN, is a neural network where any node may connect to
			any other node, and where all nodes are MPNs.
		-

	Principles

		Processing

			ASNP

				-
					By design, FFNNs should use ASNP, though the usage of them may span fixed
					cycles with defined endpoint criteria.
				-

			Training

				Linearization

					-
						An FFNN cycle may be linearized by converting the beginning and end of each
						step within it into a layer in a feed forward neural network, the end of
						one step being the start of the next.
						
						In a linearized FFNN cycle, nodes and connections may repeat as entangled
						elements throughout the network.
						
						Once in this format, the linearized FFNN cycle may be trained using some
						variation of backpropagation, after which a newly trained FFNN model may be
						reconstructed from the trained linearized FFNN cycle. This process is
						called linearized training.
					-

				Backfeeding

					-
						Backfeeding is a process for training an FFNN.

						The process shall begin by assigning the model's performance a feedback
						score, ranging from negative one to positive one, inclusive, selecting a
						set of neurons to begin the process at, ideally those serving as the output
						locations of the behavior being trained, and selecting a number of steps
						for the process to last over as a function of the number of neurons in the
						model, with the default method being to take the natural log of said number
						and square it.

						The process shall begin with the feedback score being passed to the
						selected neurons, after which said scores shall propagate backwards through
						the model in the manner of ASNP steps, altered by the weights, properties,
						and activation functions they pass through in the same manner as any other
						data, for the selected number of steps.

						Depending on the exact variant of backfeeding used, the weights and
						properties of the connections passed through may be altered as a function
						of the score values following each step. The standard method is to take the
						score value in a neuron after each step and multiply the weight of any
						outward connections of said neuron by said score value.

						As opposed to executing backfeeding over a finite number of steps, constant
						backfeeding may also be used, where feedback scores are constantly being
						passed to output nodes, and propagating backwards through the model,
						training it parallel to its usage.
					-

			Zero Augmentation

				-
					An FFNN may be grown using a modified form of zero augmentation, where a new
					node is added to the network and connected to other nodes arbitrarily, with all
					of the new parameters created from the addition of said node and its
					connections are set to zero.
				-

		Variants

			OLNN

				-
					An orbital loop neural network, or OLNN, is an FFNN with the form factor of a
					feed-forward multi-layer neural network, in which the last layer feeds back
					into the first.

					In a loose OLNN, connections may skip layers, and in a strict OLNN, they may
					not.
				-

			XNN

				-
					A cross neural network, or XNN, is an FFNN where the network is a dual absolute
					graph.
				-