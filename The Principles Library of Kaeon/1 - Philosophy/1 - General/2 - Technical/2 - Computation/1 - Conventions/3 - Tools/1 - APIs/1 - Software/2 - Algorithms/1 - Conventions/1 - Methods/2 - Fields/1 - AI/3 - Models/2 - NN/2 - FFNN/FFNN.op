Freeform Neural Network

	Philosophy

		-
			A freeform neural network, or FFNN, is a neural network where any node may connect to
			any other node, and where all nodes are MPNs.
		-

	Principles

		Processing

			ASNP

				-
					By design, FFNNs should use ASNP, though the usage of them may span fixed
					cycles with defined endpoint criteria.
				-

			Linearization

				-
					An FFNN cycle may be linearized by converting the beginning and end of each
					step within it into a layer in a feed forward neural network, the end of one
					step being the start of the next.
					
					In a linearized FFNN cycle, nodes and connections may repeat as entangled
					elements throughout the network.
					
					Once in this format, the linearized FFNN cycle may be trained using some
					variation of backpropagation, after which a newly trained FFNN model may be
					reconstructed from the trained linearized FFNN cycle. This process is called
					linearized training.
				-

			Zero Augmentation

				-
					An FFNN may be grown using a modified form of zero augmentation, where a new
					node is added to the network and connected to other nodes arbitrarily, with all
					of the new parameters created from the addition of said node and its
					connections are set to zero.
				-

		Applications

			XNN

				-
					A cross neural network, or XNN, is an FFNN where the network is a dual absolute
					graph.
				-